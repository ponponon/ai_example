from loguru import logger
import time
from transformers import AutoTokenizer, AutoModel
import os
from typing import Dict, Tuple, Union, Optional
from torch.nn import Module
from transformers import AutoModel


def auto_configure_device_map(num_gpus: int) -> Dict[str, int]:
    # transformer.word_embeddings å ç”¨1å±‚
    # transformer.final_layernorm å’Œ lm_head å ç”¨1å±‚
    # transformer.layers å ç”¨ 28 å±‚
    # æ€»å…±30å±‚åˆ†é…åˆ°num_gpuså¼ å¡ä¸Š
    num_trans_layers = 28
    per_gpu_layers = 30 / num_gpus

    # bugfix: åœ¨linuxä¸­è°ƒç”¨torch.embeddingä¼ å…¥çš„weight,inputä¸åœ¨åŒä¸€deviceä¸Š,å¯¼è‡´RuntimeError
    # windowsä¸‹ model.device ä¼šè¢«è®¾ç½®æˆ transformer.word_embeddings.device
    # linuxä¸‹ model.device ä¼šè¢«è®¾ç½®æˆ lm_head.device
    # åœ¨è°ƒç”¨chatæˆ–è€…stream_chatæ—¶,input_idsä¼šè¢«æ”¾åˆ°model.deviceä¸Š
    # å¦‚æœtransformer.word_embeddings.deviceå’Œmodel.deviceä¸åŒ,åˆ™ä¼šå¯¼è‡´RuntimeError
    # å› æ­¤è¿™é‡Œå°†transformer.word_embeddings,transformer.final_layernorm,lm_headéƒ½æ”¾åˆ°ç¬¬ä¸€å¼ å¡ä¸Š
    # æœ¬æ–‡ä»¶æ¥æºäºhttps://github.com/THUDM/ChatGLM-6B/blob/main/utils.py
    # ä»…æ­¤å¤„åšå°‘è®¸ä¿®æ”¹ä»¥æ”¯æŒChatGLM2
    device_map = {
        'transformer.embedding.word_embeddings': 0,
        'transformer.encoder.final_layernorm': 0,
        'transformer.output_layer': 0,
        'transformer.rotary_pos_emb': 0,
        'lm_head': 0
    }

    used = 2
    gpu_target = 0
    for i in range(num_trans_layers):
        if used >= per_gpu_layers:
            gpu_target += 1
            used = 0
        assert gpu_target < num_gpus
        device_map[f'transformer.encoder.layers.{i}'] = gpu_target
        used += 1

    return device_map


def load_model_on_gpus(checkpoint_path: Union[str, os.PathLike], num_gpus: int = 2,
                       device_map: Optional[Dict[str, int]] = None, **kwargs) -> Module:
    if num_gpus < 2 and device_map is None:
        model = AutoModel.from_pretrained(
            checkpoint_path, trust_remote_code=True, **kwargs).half().cuda()
    else:
        from accelerate import dispatch_model

        model = AutoModel.from_pretrained(
            checkpoint_path, trust_remote_code=True, **kwargs).half()

        if device_map is None:
            device_map = auto_configure_device_map(num_gpus)

        model = dispatch_model(model, device_map=device_map)

    return model


logger.debug(f'å¼€å§‹åŠ è½½æ¨¡å‹')
tokenizer = AutoTokenizer.from_pretrained(
    "THUDM/chatglm2-6b", trust_remote_code=True)
logger.debug(f'tokenizer down')
model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=1)
model = model.eval()
logger.debug(f'æ¨¡å‹åŠ è½½å®Œæ¯•')

tt = []

def func(filepath:str):
    with open(filepath, 'r', encoding='utf-8') as file:
        content = file.read()

    lines = [
        content
    ]

    history=[]

    for line in lines:

        text = f"""{line}"""

        s = time.time()
        response, history = model.chat(tokenizer, text, history=history)
        e = time.time()

        tt.append(e-s)

        logger.debug(text)
        logger.debug(response)
        logger.debug(f'è€—æ—¶: {round(e-s,3)} ç§’')
        print('-----------------------------------------------')

        # with open('output-30.log', 'a', encoding='utf-8') as out_file:
        #     out_file.write(f'Question ğŸ‘‰ {text}')
        #     out_file.write('\n')
        #     out_file.write(f'Answer:ğŸ‘‡\n{response}')
        #     out_file.write('\n\n-----------------------------------------------')
        #     out_file.write('\n')
        #     out_file.write('\n')
        #     out_file.write('\n')

    print(f'æ€»è€—æ—¶: {sum(tt)}ç§’')
    print(f'å¹³å‡å•ä¸ªè€—æ—¶: {sum(tt)/len(tt)}ç§’')


func('/home/pon/code/me/ai_example/examples/ChatGLM2_6B/q1.txt')